


# DeepSeek-R1 å…¨æµç¨‹å¾®è°ƒæŒ‡å—

- [å¿«é€Ÿå¼€å§‹](#L13)
- [ç¯å¢ƒé…ç½®](#L21)
- [å¾®è°ƒæµç¨‹](#L49)
- [æ¨¡å‹é‡åŒ–](#æ¨¡å‹é‡åŒ–)



## ğŸš€ å¿«é€Ÿå¼€å§‹
æœ¬ä»£ç åŒ…ä»¥å¾®è°ƒdeepseekr1-1.5bä¸ºä¾‹
```bash
git clone https://github.com/vergileZhang/deepseek_finetuning.git
cd deepseek_finetuning
```


## ğŸ”§ ç¯å¢ƒé…ç½®
### åŸå§‹ä¾èµ–å®‰è£…
```bash
pip install -r requirements.txt
```

### llama.cppç¼–è¯‘ï¼ˆåŸå§‹å‘½ä»¤ï¼‰
llama.cpp æ˜¯ä¸€ä¸ªä¸“æ³¨äºåœ¨è¾¹ç¼˜è®¾å¤‡ã€ä¸ªäººPCä¸Šè¿›è¡Œllméƒ¨ç½²çš„é«˜æ€§èƒ½æ¨ç†æ¡†æ¶ã€‚
```bash
git clone https://github.com/ggerganov/llama.cpp 
cd llama.cpp
```
è¿è¡Œä¸‹è¡Œæ— æŠ¥é”™ï¼Œä»¥ç¡®ä¿æ‚¨çš„CUDA Toolkitså’ŒCMakeè®¾ç½®æ­£ç¡®

```bash
nvcc -version
cmake --version
```
é‡‡ç”¨CUDAåŠ é€Ÿç¼–è¯‘
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config
```
é…ç½®llama.cppç¯å¢ƒ
```bash
pip install -r requirements.txt
```

## ğŸ› ï¸ å¾®è°ƒæµç¨‹
### 1. æ•°æ®é¢„å¤„ç†
```bash
# ä¿æŒåŸå§‹å¤„ç†æµç¨‹ä¸å˜
# æ•°æ®é›†æ ¼å¼è¦æ±‚ï¼š
# - 500æ¡æ³•å¾‹é—®ç­”
# - JSONæ ¼å¼
# - åŒ…å«prompt/responseå­—æ®µ
```

### 2. æ‰§è¡Œå¾®è°ƒ
```bash
python train.py \
--base_model_path /path/to/your/base_model \
--dataset_path /path/to/your/dataset \
--lora_adapter_path /path/to/save/LoRA_adapter \
--model_path /path/to/save/finetuned_model
```

#### å‚æ•°è¯´æ˜
| å‚æ•° | åŸå§‹è¯´æ˜ |
|------|----------|
| `--base_model_path` | åŸå§‹å­˜æ”¾åŒ…å«`.safetensors`æ–‡ä»¶çš„æ¨¡å‹ç›®å½• |
| `--dataset_path` | å¾®è°ƒæ•°æ®é›†å­˜æ”¾è·¯å¾„ | 
| `--lora_adapter_path` | å¾®è°ƒäº§ç”Ÿçš„ä½ç§©çŸ©é˜µå‚æ•°ï¼Œè€Œéå®Œæ•´çš„æ¨¡å‹æƒé‡ |
| `--model_path` | å¾®è°ƒåçš„å®Œæ•´å‚æ•°ä¿å­˜è·¯å¾„ |


## ğŸ—œï¸ æ¨¡å‹é‡åŒ–
### è½¬GGUF
åŸå§‹æ¨¡å‹æƒé‡ä¸º`.safetensors`æ–‡ä»¶ï¼Œé¡»è½¬`.GGUF`åæ‰å¯ä¾›OLLAMAéƒ¨ç½²
```bash
cd llama.cpp
python convert_hf_to_gguf.py /path/to/save/finetuned_model --outfile /path/to/save/GGUF
```

### é‡åŒ–
```bash
./llama-quantize /path/to/save/GGUF /path/to/save/your_model-***.gguf ***
```
å¾—åˆ°æ–‡ä»¶`your_model-***.gguf`ï¼Œå…¶ä¸­ï¼Œ`***`ä¸ºéœ€è¦æ‚¨æŒ‡å®šçš„é‡åŒ–ç¨‹åº¦ï¼Œæœ‰ä¸‹åˆ—å¤šä¸ªæ ‡å‡†å¯ä»¥é€‰æ‹©ã€‚é€‰æ‹©ä¸åŒçš„é‡åŒ–ï¼Œæ¨¡å‹çš„æ¨ç†æ•ˆæœä¸ä¸€æ ·

 - `q2_K`
 - `q3_K`
 - `q3_K_S`
 - `q3_K_M`
 - `q3_K_L`
 - `q4_0`
 - `q4_1`
 - `q4_K`
 - `q4_K_S`
 - `q4_K_M`
 - `q5_0`
 - `q5_1`
 - `q5_K`
 - `q5_K_S`
 - `q5_K_M`
 - `q6_K`
 - `q8_0`
 - `f16`


## ğŸ“¦ OLLAMAéƒ¨ç½²
### Modelfile
åˆ›å»ºModelfileæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
```dockerfile
FROM /path/to/save/quantized_GGUF
    
TEMPLATE """
{{ if .System }}<|system|>
{{ .System }}</s>{{ end }}
    
<|user|>
{{ .Prompt }}</s>
    
<|assistant|>
{{ .Response }}
"""
    
SYSTEM "You are a helpful assistant."
PARAMETER temperature 0.7
PARAMETER repeat_penalty 1.2
```

### å¯åŠ¨å‘½ä»¤
```bash
ollama serve
ollama create your_model_name -f path/to/Modelfile
ollama run your_model_name
```




## ğŸ“š å‚è€ƒæ–‡æ¡£
> Blockquote
> åŸå§‹å‚è€ƒæ–‡çŒ®å’Œå¼•ç”¨è¯´æ˜


